{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# クラスタリング\n",
    "\n",
    "# 目的：関連のある文章を見つける\n",
    "\n",
    "# 前章まで => 教師あり学習\n",
    "#     * ラベル付きデータの取得が困難な場合もある\n",
    "#     * 人がラベルを付けるのは非常にコスト\n",
    "\n",
    "# 教師なし学習\n",
    "#     * データだけからパターンを見つけ出すことが可能\n",
    "\n",
    "# 例\n",
    "#     * Q&Aサイトを考える\n",
    "#     * ユーザが探している情報（キーワード）を検索すると、ふさわしいページを表示する\n",
    "#     * 探している回答でない場合、関連した質問を表示したい\n",
    "\n",
    "# 単純なアプローチ\n",
    "#     * すべての類似度を算出 => コストが非常に高い\n",
    "\n",
    "# クラスタリング\n",
    "#      * クラスタ => データの集合\n",
    "#      * 似ているものは同じクラスタ、似てないものは別のクラスタに\n",
    "#      * 今回の場合、テキストデータ同士で類似度を算出する方法が必要\n",
    "#      * SciKit ライブラリを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.1 文書の関連性を計測する\n",
    "\n",
    "# テキストデータのままでは計測は困難\n",
    "# テキストデータを数字（ベクトル）に変換することが必要 => 類似度の計算が可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.1.1 やってはいけないこと\n",
    "\n",
    "# 類似度を求める手法 => レーベンシュタイン距離（編集距離）\n",
    "# レーベンシュタイン距離\n",
    "#      * 編集を行う最小回数\n",
    "#      * 「machine」と「mchiene」 => 距離は 2\n",
    "#      * これを求める計算コストは高い\n",
    "#      * 単語レベルの編集距離は考えるべきでない\n",
    "\n",
    "# 応用\n",
    "#     * 文章全体で編集距離を計測（単語ごと）\n",
    "#     * 「How to format my hard disk」と「Hard disk format problems」 => 距離 6\n",
    "#     * 文字レベルより早いが、依然として時間かかる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.1.2 どうやるべきか\n",
    "\n",
    "# bag-of-words\n",
    "#     * 単語の出現回数を特徴量として用いる => 結果をベクトル化\n",
    "#     * ベクトルのサイズが大きくなりがち\n",
    "#     * ベクトル化することでユーグリット距離の計算が可能 => 最近傍点を発見\n",
    "#     * 時間がかかる\n",
    "\n",
    "# 手順\n",
    "#     1. 各文書から特徴量を抽出し、ベクトルの形で保存\n",
    "#     2. 特徴ベクトルに対してクラスタリングを行う\n",
    "#     3. 投稿された質問書に対して、クラスタを決定\n",
    "#     4. このクラスタに属する文書を他に集め、多様性を増す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2 前処理\n",
    "\n",
    "# 共通する単語の出現回数を類似度として計測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# 3.2.1 テキストデータを bag-of-words に変換する\n",
    "\n",
    "# ライブラリをインポートするだけ\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)\n",
    "\n",
    "# パラメータ min_dif : 頻繁には使われていない単語を無視する（ここでは一回未満）\n",
    "# print することで、デフォルトのパラメータを表示\n",
    "\n",
    "# analyzer='word'  : 単語レベルで出現回数をカウント\n",
    "# token_pattern : 単語の決定方法を正規表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "\n",
    "# 数えるべき単語を抜き出す\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# それぞれの文書が数えるべき単語を何個含んでいるかを表すベクトル\n",
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "Imaging databases provide storage capabilities.\n",
      "Most imaging databases save images permanently.\n",
      "\n",
      "Imaging databases store data.\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n"
     ]
    }
   ],
   "source": [
    "# 3.2.2 単語を数える\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "DIR = './data/toys'\n",
    "\n",
    "# DIR 以下にある txt ファイルを読み込む\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in sorted(os.listdir(DIR))]\n",
    "\n",
    "for post in posts:\n",
    "    print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "# vectorizer のパラメータに従い、ベクトル化\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "# サンプル数と、数えるべき単語数を取得\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "# 数えるべき単語のラベルを取得\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 新しい文書をベクトル化\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# ほとんどの要素が 0 のベクトルになる\n",
    "# そのため、出現した単語のみの情報だけ格納する\n",
    "print(new_post_vec)\n",
    "print()\n",
    "\n",
    "# toarray() を用いることで、特徴ベクトルのすべての要素を表示できる（サンプル数が増えると膨大になる）\n",
    "print(new_post_vec.toarray())\n",
    "print()\n",
    "\n",
    "# 特徴ベクトルの長さ\n",
    "print(len(new_post_vec.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 類似度を求める\n",
    "# 簡単な手法として、ユーグリット距離を用いる\n",
    "# 計算するためには、toarray() を用いて、特徴ベクトルの形式にする\n",
    "# ユーグリットノムル（ユーグリット距離）は scipy の norm 関数で計算できる\n",
    "\n",
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "# 実際に計算してみる\n",
    "# \n",
    "\n",
    "import sys\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "# サンプル数（入力する文書数）回ループ\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    # 特徴ベクトルから、出現した単語のみの情報だけ格納する\n",
    "    post_vec = X_train.getrow(i)\n",
    "    # print(post_vec)\n",
    "    \n",
    "    # new_post とのユーグリット距離を計算\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    # 類似度が高い場合値を更新\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "# 結果\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "# 考察\n",
    "# 文書0 が最も似ていない => 共通する単語なし　（合ってる？）\n",
    "# 文書1 と文書3 は共に新しい文書の単語を含み、類似度も高い。しかし、文書3 のが文書が短いため、最も類似していると言える\n",
    "# 文書3,4 は繰り返してるだけ。本来同じ類似度なはずだけど…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 出現回数だけ見ると、ユーグリット距離が大きくなってしまう\n",
    "# => これだけでは単純すぎ\n",
    "# 特徴ベクトルの単位長さにするために正規化が必要\n",
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.3 単語の出現回数ベクトルを正規化する\n",
    "\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    \n",
    "#     print(v1_normalized)\n",
    "#     print(v2_normalized)\n",
    "\n",
    "    delta = v1_normalized - v2_normalized\n",
    "\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "# 正規化して計算してみる\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "# サンプル数（入力する文書数）回ループ\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    # 特徴ベクトルから、出現した単語のみの情報だけ格納する\n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    # new_post とのユーグリット距離を計算\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    # 類似度が高い場合値を更新\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "# 結果\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "# 考察\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "# 3.2.4 重要度の低い単語を取り除く\n",
    "\n",
    "# ストップワード（多くの文書に出現し、情報をあまり持たない単語）を設定する\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "# ストップワード例（一部）\n",
    "print(sorted(vectorizer.get_stop_words())[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 18\n"
     ]
    }
   ],
   "source": [
    "# ストップワードを用いると、単語リストが 7 つ減った\n",
    "\n",
    "# vectorizer のパラメータに従い、ベクトル化\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "# サンプル数と、数えるべき単語数を取得\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "# 新しい文書をベクトル化\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "# 同様に、類似度を計算\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "# サンプル数（入力する文書数）回ループ\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    # 特徴ベクトルから、出現した単語のみの情報だけ格納する\n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    # new_post とのユーグリット距離を計算\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    # 類似度が高い場合値を更新\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "# 結果\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "# 考察\n",
    "# ここではあまり大きな変化は見られなかったが、重要\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.5 ステミング\n",
    "\n",
    "# 意味的には同じ単語が語形変形により異なる形でカウントされてしまう\n",
    "# imaging と　images は同じ単語としてカウントするほうが理にかなっている\n",
    "# NLTK (Natural Language Toolkit) を用いることで、CountVectorizer のプラグインとして実現可能（日本語ver 要調査）\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "# ステミング例\n",
    "\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "print(s.stem(\"graphics\"))\n",
    "print(s.stem('imaging'))\n",
    "print(s.stem('image'))\n",
    "print(s.stem('imagination'))\n",
    "print(s.stem('imagine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy\n",
      "buy\n",
      "bought\n"
     ]
    }
   ],
   "source": [
    "# ステミングは必ずしも正しいとは限らない\n",
    "\n",
    "print(s.stem('buys'))\n",
    "print(s.stem('buying'))\n",
    "print(s.stem('bought'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "# NLTK のステマーを用いてベクトル化を拡張する\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        # これよくわからない\n",
    "        # build_analyzer 調べる\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        # それぞれの単語をステマーして格納\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "        \n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "# vectorizer のパラメータに従い、ベクトル化\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "# サンプル数と、数えるべき単語数を取得\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "# 新しい文書をベクトル化\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "# 同様に類似度を求める\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "# サンプル数（入力する文書数）回ループ\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    # 特徴ベクトルから、出現した単語のみの情報だけ格納する\n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    # new_post とのユーグリット距離を計算\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    # 類似度が高い場合値を更新\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "# 結果\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "# 考察\n",
    "# 文書2 が image を２つ含むようになるため、最も類似度が高くなった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.6 TF-IDF を用いる\n",
    "\n",
    "# これまで：　出現頻度 => 重要度\n",
    "# どの文書にも存在するような単語も存在するため、十分でない\n",
    "# max_df=0.9 とすることで、すべての文書の 90 % 以上に出現する単語を除外できるが、89 % の場合は？\n",
    "# max_df をどのように設定すればいいか問題\n",
    "\n",
    "# TF-IDF\n",
    "# 特定の文書だけで現れやすい単語、つまり、他の文書ではあまり現れない単語は、その文書において特徴量が大きい\n",
    "\n",
    "# 手法\n",
    "# ➀ ある単語に対して、対象の文書中で出現した回数をカウント\n",
    "# ➁ その単語が他の文書でどれだけ出現するかをカウント\n",
    "# ➂ ➀ / ➁\n",
    "# \n",
    "#  多く出現するほど値は大きくなる　かつ　他の文書で多く出現すると値は小さくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 実装\n",
    "\n",
    "import math\n",
    "import scipy as sp\n",
    "\n",
    "def tfidf(term, doc, docset):\n",
    "    tf = float(doc.count(term) / sum(doc.count(w) for w in set(doc)))\n",
    "    idf = math.log(float(len(docset)) / (len([doc for doc in docset if term in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.27031007207210955\n",
      "0.0\n",
      "0.13515503603605478\n",
      "0.3662040962227032\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "doc_a, doc_abb, doc_abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [doc_a, doc_abb, doc_abc]\n",
    "print(tfidf(\"a\", doc_a, D))\n",
    "print(tfidf(\"b\", doc_abb, D))\n",
    "print(tfidf(\"a\", doc_abc, D))\n",
    "print(tfidf(\"b\", doc_abc, D))\n",
    "print(tfidf(\"c\", doc_abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a はすべての文書で用いられるため、意味を持たない\n",
    "# b は doc_abb の文書にとって 、doc_abc の２倍重要\n",
    "# c は doc_abc にとって最も特徴量が大きい単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF も scikitlearn で利用可能\n",
    "# TfidfVectorizer 　(CountVectorizer を継承したクラス) に含まれる\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "        \n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "# vectorizer のパラメータに従い、ベクトル化\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "# サンプル数と、数えるべき単語数を取得\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "# 新しい文書をベクトル化\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.92: Imaging databases store data.\n",
      "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.86\n"
     ]
    }
   ],
   "source": [
    "# 同様\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "# サンプル数（入力する文書数）回ループ\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    # 特徴ベクトルから、出現した単語のみの情報だけ格納する\n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    # new_post とのユーグリット距離を計算\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    # 類似度が高い場合値を更新\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "# 結果\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.7 テキストデータの前処理としてここまでやってきたこと\n",
    "\n",
    "# 1. テキストデータをトークン化\n",
    "# 2. 頻出しすぎる単語は、関連する文書を見つけるために役に立たないため除外\n",
    "# 3. めったに使われない単語は、新しい文書でも使われる可能性が低いため除外\n",
    "# 4. 残った単語について、その出現回数をカウント\n",
    "# 5. 文書全体の状況を考慮するため、単語の出現回数から TF-IDF を計算\n",
    "\n",
    "# ノイズのあるテキストデータから、簡潔にまとめられた特徴量を得ることが可能\n",
    "\n",
    "# bag-of-words は単純だが、優れた性能（ただし、欠点も多い）\n",
    "# 欠点\n",
    "# 「Car hits wall」 と　「Wall hits car」 は同じ特徴ベクトル\n",
    "# 「I will eat ice cream」 と　「I will not eat ice cream」 の特徴ベクトルは意味は逆だが非常に近い\n",
    "# （上は、２つの単語のペアや３つの単語のペアを一つのまとまりとしてカウントすることで解決可能）\n",
    "# タイポには対応不可\n",
    "\n",
    "# ここまでで、効率的にクラスタリングを行う用意が完了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
